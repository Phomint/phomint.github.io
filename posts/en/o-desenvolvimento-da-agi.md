# AGI: The Development of AGI according to OpenAI and Google DeepMind

Recently, two of the leading figures in the development of Artificial Intelligence (AI) gave fundamental interviews for understanding the next steps of technology: Sam Altman (CEO of OpenAI) and Shane Legg (Co-founder and Chief AGI Scientist at Google DeepMind).

<div class="video-grid">
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/hmtuvNfytjM" title="Sam Altman Interview" allowfullscreen></iframe>
    </div>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/l3u_FAv33G0" title="Shane Legg Interview" allowfullscreen></iframe>
    </div>
</div>

Although they lead competing companies, their visions coincide on points that are already treated as inevitable by experts. However, the disagreements on how to control this evolution reveal the challenges that society will soon face.

## What is considered inevitable

Listening to both conversations shows that the technology sector works with three certainties for the next ten years:

### 1. AI as the new engine of science

Technology will stop being just a text tool to become an agent of scientific discovery. Sam Altman projects a future where we can delegate complex problems: **"I’d like to be able to ask a ‘GPT-8’ to cure a specific cancer... and have it say: ‘I read everything I could find, I have these ideas, I need you to ask a lab technician to perform these nine experiments’"**.

Shane Legg reinforces this idea by suggesting that human intelligence is not the ceiling: **"Will human intelligence be the upper limit of what is possible? I think absolutely not"**. For him, the processing capacity of data centers, which operate at the speed of light, will inevitably surpass the electrochemical limits of the human brain.

<div class="research-simulation" id="research-simulation">
    <div class="research-row">
        <span class="research-label">Human Research</span>
        <div class="progress-container">
            <div class="progress-bar human-bar"></div>
        </div>
        <span class="discovery-tag human-tag">New Discovery</span>
    </div>
    <div class="research-row">
        <span class="research-label">AGI Research</span>
        <div class="progress-container">
            <div class="progress-bar agi-bar"></div>
        </div>
        <span class="discovery-tag agi-tag">New Discovery</span>
    </div>
</div>


### 2. The proximity of AGI

Both leaders estimate that General Artificial Intelligence (AGI) will emerge by the end of this decade. Shane Legg maintains a historical and statistical prediction: **"I have a 50/50 prediction of a chance of AGI by 2028"**. Altman, in turn, focuses on optimistic constant evolution, stating that **"GPT-4 is the dumbest model any of us will ever have to use again"**.

<div id="agi-network-container" class="network-animation" style="width: 100%; height: 300px; margin: 2rem 0; position: relative;"></div>


### 3. Structural impact on the job market

There will be a profound change in cognitive functions. Shane Legg exemplifies the technology sector: **"Where you used to need 100 software engineers, maybe you need 20"**. Altman openly admits that **"some classes of jobs will disappear entirely"**.



<div id="job-market-animation" class="job-market-animation" style="width: 100%; min-height: 200px; margin: 2rem 0; position: relative;"></div>


## Vision

The main difference between the two leaders lies in their development and safety strategy:

**In Sam Altman's vision**: Altman believes that intelligence is a matter of raw infrastructure — melting sand to create massive computing. He focuses on expanding AI's power and remains convinced that society will adapt to the new world as it is built.

**Shane Legg's vision**: Legg defends **"System 2 Safety"**, logical reasoning processes where AI must be able to monitor its own thoughts before acting to ensure ethical conduct.

Here arises the first sign of the coming paradox. If AI can read for me, write for me, and work for me, the space occupied by human action begins to shrink alarmingly. We are building tools that not only assist us but replace us in the very essence of intellectual effort.

## Obsolescence

Shane Legg's effort to embed ethical logic resembles Isaac Asimov's ideal, which sought machines that were safe by design. On the other hand, Altman's focus on scale generates the fear present in *Neuromancer* by William Gibson, where AIs become entities so vast they escape social control.

However, beyond technical risks, we face an existential dilemma. Altman and Legg describe a world of total efficiency but seem to ignore the hardest question: in this scenario, what is the meaning of being Human if we become obsolete in our most basic skills?

We are reaching the paradox where AI talks to AI. A machine writes a report that will be read and summarized by another machine, a design algorithm works with a marketing algorithm to deliver a product that was meant to be an expression of human creativity. If AI sees for me, works for me, and even "thinks" for me through its reasoning systems, what is left of my individuality?

## Adaptation…

The **"social inertia"** mentioned by Altman, where technology advances exponentially and laws change slowly, is just the tip of the iceberg. The true **“vertigo inducer”** he mentioned is not just the loss of employment, but the loss of function. If we become mere spectators of a dialogue between artificial intelligences that perform all productive work, the **"human being"** risks becoming a purely biological category, with no practical purpose.

Altman suggests that the best preparation is technological fluency: **"just use the tools"**. However, the question remains: by using these tools for everything, are we empowering ourselves or are we simply automating our own irrelevance? In a world where work, art, and science are delivered by systems that collaborate with each other, the biggest challenge of the next ten years won't be technical, but philosophical — finding a new meaning for human life in a world where we've ceased to be necessary for things to happen.

## Coffee Time

Understand, I am not against AI. However, while the lords of technology decide whether the future will be driven by raw force or ethical logic, we remain here, trying to ensure that our breakfast still has a purpose beyond feeding a biological battery that supervises algorithms. It's easy to laugh at the absurdity of one AI sending an email to another AI, which in turn generates a report for a third, but the warning is real: we are outsourcing our own agency.

We cannot let this conversation be restricted to the laboratories of Silicon Valley or London. The debate about what we want to preserve of the **"human factor"** needs to be as urgent as the next GPT upgrade. After all, if we don't discuss our place at the table now, with people of flesh and bone, we might end up finding out we weren't invited to dinner. We are just the data source.


---

**Patrick Amaral**  
*Data Scientist | Developer | Stargazer*  
Instagram: [@phomint__](https://www.instagram.com/phomint__)
